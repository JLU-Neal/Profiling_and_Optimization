Task 1:

Top Hotspots:
Function	                                           Module              	CPU Time
tech::tablesaw::table::StandardTableSliceGroup::splitOn[Compiled Java code]	2.227s
tech::tablesaw::joining::DataFrameJoiner::crossProduct[Compiled Java code]	1.742s
...


I used hardware event-based sampling, which is supposed to be more accurate, deterministic
semantic and low noise (according to the Intel, the average overhead of event-based sampling
is about 2% on a 1ms sampling interval.) 



I determined the Hotspots simply by its CPU time. Function 
tech::tablesaw::table::StandardTableSliceGroup::splitOn comprises the majority of the runtime.  

Task 2:




To reproduce the behaviour of the function without overhead, we directly call the function 
splitOn based on an existing table which is initialized by a @Setup function.


Since the splitOn function will iterate every row of the table, it is intuitively that 
the excecution time would increase linearly as the number of the rows growing.

However, my hypothesis on the impact of columns could be counterintuitive: 
the performance of benchmark would not be changed a lot by simply scaling up the number of columns, 
what really matter to the performance is the number of columns which will be splitted on.

In detail, I think the number of colums to be splitted on would determine the "workload" of 
each row during the iteration (to generate the key for hashmap), while rest of the columns 
would not since the whole row would be simply referenced in the value of the hashmap.

the hypothesis could be stated as:

The running time will increase linearly as the number of the rows in table and the number of 
colums which would be split on (not the number of columns) grow.

in the form of equation:

T_exe = O(Rows * ColsToBeSplittedOn)

where T-exe denote the excecution time of the benchmark, O(.) is the big O notation, Rows is 
the number of the rows of the table, ColsToBeSplittedOn is the number of the columns to be 
splitted on. 

The hypothesis could be validated from the result of the performance. 

Benchmark               (cols)  (colsToBeSplitedOn)  (rows)           Score        

variable:rows
MyBenchmark.testMethod       1                    1      10        1956.592
MyBenchmark.testMethod       1                    1     100       15715.194
MyBenchmark.testMethod       1                    1    1000      155006.353 
MyBenchmark.testMethod       1                    1   10000     1556853.175

variable:cols
MyBenchmark.testMethod      10                    1      10        1937.535 
MyBenchmark.testMethod     100                    1      10        2039.938 

variable:colsToBeSplitedOn
MyBenchmark.testMethod     100                   10      10       11068.935 
MyBenchmark.testMethod     100                  100      10      204744.664

When the only variable is rows or colsToBeSplitedOn, the T_exe grows linearly as rows scaling 
up;

When the only variable is cols, there is no significant changes of T_exe as cols scaling up;


Task 3:

Benchmark                       Time     Iterations   Percentage of Java counterpart   
DummyBenchmark/1/10          1353 ns         508476   69.1%
DummyBenchmark/1/100         5489 ns         125086   34.9%
DummyBenchmark/1/1000       44451 ns          15761   28.6%
DummyBenchmark/1/10000     436295 ns           1596   28.0%
(Here we only focus on the impace of number of rows, while the colsToBeSplitedOn is set to 1)


From the performance of our C++ implementation, it can be noticed that the percentage of C++ 
implementation excecution time to its Java counterpart is getting slower while the number of 
row scaling up.

This could be explained that when the rows is reletive smaller, higher percentage of time 
would be spent to deal with some overhead.




